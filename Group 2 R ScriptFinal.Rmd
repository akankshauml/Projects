---
title: Group 2 Project 
Team Members: Akanksha Jain, Serge Smirnov, Lucky Nnyamah, Bora Chhun 
Dataset: Titanic Training Data from Kaggle
---
Project Abstract 


Loading the necessary libraries
```{r}
library(ggplot2)
library(tidyverse)
library(class)
library(MASS)
library(caret)
setwd("/Users/akankshajain/Documents/MSBA/POMS6120/Project")
```

Data Preparation

Load data and delete rows with null values. 
Age has 177 null values and Embarked 2 null values. 
No outliers in the dataset

```{r}
#Loading the dataset 
titanic_data <- read.csv( "Titanic_data.csv",header=TRUE, sep=",", na.strings="")

#missing per row and column
#rowSums(is.na(titanic_data))
colSums(is.na(titanic_data))

# deleting rows with null values
titanic_data = na.omit(titanic_data)
dim(titanic_data)
colSums(is.na(titanic_data))
#Rounding Fare to remove decimal
titanic_data$Fare<-round(titanic_data$Fare)
```

Creating a training and testing set with 80%-20% split. Setting seed to ensure consistend results 
Performed some descriptive analysis on the sets for data understanding
```{r}
# Splitting data in 80% training and 20% testing/validating set
n = nrow(titanic_data)
set.seed(6120)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
train = titanic_data[trainIndex ,]
test = titanic_data[-trainIndex ,]
# Dimension of train and test sets
dim(train)
dim(test)

#Summary of train and test data
summary(train)

#Variable class
sapply(train,class)

#correlation
cor(train[,c(-1,-4,-5,-9,-11)])


#subsetting only Survival column
survival_train <- train[,c(2) ]
survival_test <- test[,c(2) ]
```
Predictive modeling 

Logistic Regression

Fare and Pclass are highy correlated, First we will use only PClass in Model 1, 2

Model 1 : Variables Pclass, Sex, Age, SibSp, Parch, Embarked. Accuracy is 83.09 % 
The P values for embarked and Parch are high which suggests that changes in the predictor variable are not associated with changes in the response variable. Will not include Parch and Embarked in Model 2
```{r}
x1 = as.factor(Survived) ~ factor(Pclass) + Sex + Age + SibSp + Parch + Embarked

glm.fits1 = glm( x1, data = train, family = binomial)
summary(glm.fits1)

glm.probs1= predict(glm.fits1,test, type = "response")
glm.pred1 = rep(0, nrow(test))
glm.pred1[glm.probs1 > .5] = 1
mean(glm.pred1 == survival_test)
GLM_Matrix_1 = table(glm.pred1, survival_test)
GLM_Matrix_1

sensitivity(GLM_Matrix_1)
specificity(GLM_Matrix_1)

#3 Examples of the model prediction

#Jack
Leo = test[22,]
Leo$Survived = 0
Leo$Pclass = 3
Leo$Sex = "male"
Leo$Age = 22
Leo$SibSp = 0
Leo$Parch = 0
Leo$Embarked = "S"
Leo_Test = 0

glm.Leo = predict(glm.fits1,Leo, type = "response")
glm.predLeo = rep(0, nrow(Leo))
glm.predLeo[glm.Leo > .5] = 1
glm.predLeo == Leo_Test

#Rose
Rose = test[22,]
Rose$Survived = 1
Rose$Pclass = 1
Rose$Sex = "female"
Rose$Age = 17
Rose$SibSp = 1
Rose$Parch = 1
Rose$Embarked = "S"
Rose_Test = 1

glm.Rose = predict(glm.fits1,Rose, type = "response")
glm.predRose = rep(0, nrow(Rose))
glm.predRose[glm.Rose > .5] = 1
glm.predRose == Rose_Test

#Cal
Cal = test[22,]
Cal$Survived = 1
Cal$Pclass = 1
Cal$Sex = "male"
Cal$Age = 30
Cal$SibSp = 0
Cal$Parch = 0
Cal$Embarked = "S"
Cal_Test = 1

glm.Cal = predict(glm.fits1,Cal, type = "response")
glm.predCal = rep(0, nrow(Cal))
glm.predCal[glm.Cal > .5] = 1
glm.predCal == Cal_Test




```

Model 2 : Variables Pclass, Sex, Age, SibSp . Accuracy is 82.39%  
```{r}
x2 = as.factor(Survived) ~ as.factor(Pclass) + Sex + Age + SibSp
glm.fits2 = glm( x2, data = train, family = binomial)
summary(glm.fits2)

glm.probs2= predict(glm.fits2,test, type = "response")
glm.pred2 = rep(0, nrow(test))
glm.pred2[glm.probs2 > .5] = 1
mean(glm.pred2 == survival_test)
GLM_Matrix_2 = table(glm.pred2, survival_test)
GLM_Matrix_2

sensitivity(GLM_Matrix_2)
specificity(GLM_Matrix_2)

```

Let us run the model with Fare instead of Pclass
Model 3 : Variables Fare, Sex, Age, SibSp, Parch, Embarked . Accuracy 80.98%
Embarked had a high P value, Re-run the model 4 without embarked
```{r}
x3 = as.factor(Survived) ~ Fare + Sex + Age + SibSp + Parch + Embarked
glm.fits3 = glm(x3, data = train, family = binomial)
summary(glm.fits3)

glm.probs3= predict(glm.fits3,test, type = "response")
glm.pred3 = rep(0, nrow(test))
glm.pred3[glm.probs3 > .5] = 1
mean(glm.pred3 == survival_test)
GLM_Matrix_3 = table(glm.pred3, survival_test)
GLM_Matrix_3

sensitivity(GLM_Matrix_3)
specificity(GLM_Matrix_3)
```

Model 4 : Variables Fare, Sex, Age, SibSp, Parch . Accuracy is same as model 3 :80.98%
```{r}
x4 = as.factor(Survived) ~ Fare + Sex + Age + SibSp + Parch
glm.fits4 = glm(x4 , data = train, family = binomial)
summary(glm.fits4)

glm.probs4= predict(glm.fits4,test, type = "response")
glm.pred4 = rep(0, nrow(test))
glm.pred4[glm.probs4 > .5] = 1
mean(glm.pred4 == survival_test)
GLM_Matrix_4 = table(glm.pred4, survival_test)
GLM_Matrix_4

sensitivity(GLM_Matrix_4)
specificity(GLM_Matrix_4)
```


QDA

QDA with PClass, Accuracy 78.87%

```{r}
qda.fit = qda(x1, data = train)
qda.fit

qda.pred = predict(qda.fit, test)$class
QDA_Matrix_1 = table(qda.pred, survival_test)
QDA_Matrix_1

sensitivity(QDA_Matrix_1)
specificity(QDA_Matrix_1)

mean(qda.pred == survival_test)
```


QDA with Fare Accuracy 81.69%%
```{r}
qda.fit = qda(x3, data = train)
qda.fit

qda.pred = predict(qda.fit, test)$class
QDA_Matrix_2 = table(qda.pred, survival_test)
QDA_Matrix_2

sensitivity(QDA_Matrix_2)
specificity(QDA_Matrix_2)

mean(qda.pred == survival_test)
```


KNN - Data Prep
```{r}

library(class)
names(titanic_data)

titanic_data$Pclass = scale(titanic_data$Pclass)
titanic_data$SibSp = scale(titanic_data$SibSp)
titanic_data$Fare = scale(titanic_data$Fare)
titanic_data$Age = scale(titanic_data$Age)
titanic_data$Parch = scale(titanic_data$Parch)

dim(titanic_data)
summary(titanic_data)

attach(titanic_data)

```

KNN model with variables: Pclass, Parch, SibSp, Age
k=23 has the best KNN results  Accuracy 64.08%
```{r}

train.x = cbind( as.factor(Pclass), Parch, SibSp, Age)[1:nrow(train),]
test.x = cbind( as.factor(Pclass), Parch, SibSp, Age)[(nrow(train) +1):nrow(titanic_data),]

set.seed(17)
cv.error.50 = rep(0,50)
for (i in 1:50) {
  knn.pred = knn(data.frame(train.x), data.frame(test.x), survival_train, k=i)
  cv.error.50[i] = mean(knn.pred == survival_test)
}

which.max(cv.error.50)

knn.pred1 = knn(data.frame(train.x), data.frame(test.x), survival_train, k=which.max(cv.error.50))
KNN_Matrix_1 = table(knn.pred1, survival_test)
KNN_Matrix_1

sensitivity(KNN_Matrix_1)
specificity(KNN_Matrix_1)

mean(knn.pred1 == survival_test)
```


KNN model with variables: Pclass, Parch, SibSp, Age, Fare, Sex
k=48 has the best KNN results  Accuracy 62.67%
```{r}
train.x = cbind( as.factor(Pclass), Parch, SibSp, Age, Fare, as.factor(Sex))[1:nrow(train),]
test.x = cbind(  as.factor(Pclass), Parch, SibSp, Age, Fare, as.factor(Sex))[(nrow(train) +1):nrow(titanic_data),]

set.seed(18)
cv.error.50 = rep(0,50)
for (i in 1:50) {
  knn.pred = knn(data.frame(train.x), data.frame(test.x), survival_train, k=i)
  cv.error.50[i] = mean(knn.pred == survival_test)
}

which.max(cv.error.50)

knn.pred1 = knn(data.frame(train.x), data.frame(test.x), survival_train, k=which.max(cv.error.50))
KNN_Matrix_2 = table(knn.pred1, survival_test)
KNN_Matrix_2

sensitivity(KNN_Matrix_2)
specificity(KNN_Matrix_2)

mean(knn.pred1 == survival_test)

```


KNN model with variables: Pclass, Parch, SibSp, Age, Fare
k=33 has the best KNN results  Accuracy 61.97%
```{r}
train.x = cbind( as.factor(Pclass), Parch, SibSp, Age, Fare)[1:nrow(train),]
test.x = cbind( as.factor(Pclass), Parch, SibSp, Age, Fare)[(nrow(train) +1):nrow(titanic_data),]

set.seed(19)
cv.error.50 = rep(0,50)
for (i in 1:50) {
  knn.pred = knn(data.frame(train.x), data.frame(test.x), survival_train, k=i)
  cv.error.50[i] = mean(knn.pred == survival_test)
}

which.max(cv.error.50)

knn.pred1 = knn(data.frame(train.x), data.frame(test.x), survival_train, k=which.max(cv.error.50))
KNN_Matrix_3 = table(knn.pred1, survival_test)
KNN_Matrix_3

sensitivity(KNN_Matrix_3)
specificity(KNN_Matrix_3)

mean(knn.pred1 == survival_test)
```

CV Logistic Regression

```{r}
library(e1071)

train_control<- trainControl(method="cv", number=10)
set.seed(123)
model1<- train(x1, data=titanic_data, trControl=train_control, method="glm", family=binomial())
model1

set.seed(124)
model2<- train(x2, data=titanic_data, trControl=train_control, method="glm", family=binomial())
model2

set.seed(125)
model3<- train(x3, data=titanic_data, trControl=train_control, method="glm", family=binomial())
model3

set.seed(126)
model4<- train(x4, data=titanic_data, trControl=train_control, method="glm", family=binomial())
model4
```


Cross Validation QDA
Accuracy:
Model 1 77.69% 
Model 2 78.10%

```{r}

set.seed(129)
model1a<- train(x1, data=titanic_data, trControl=train_control, method="qda", family=binomial())
model1a

set.seed(128)
model2a<- train(x3, data=titanic_data, trControl=train_control, method="qda", family=binomial())
model2a

```




